===================
carray User's Guide
===================

Introduction
============

carray at glance
----------------

carray is a Python package that provides containers for numerical data
that can be compressed in-memory.  It is highly based on NumPy, and
uses it as the standard way to communicate with carray objects.

The building blocks of carray objects are the so-called ``chunks``
that are compressed as a whole, but they can be decompressed partially
in order to improve the fetching of small parts of the array.

This ``chunked`` nature of the carray objects, together with a buffer
for performing I/O, makes appends very cheap and fetches reasonably
fast.  However, modification of values is an expensive operation.

The compresion/decompression process is carried out internally by
Blosc, a high-performance compressor that is optimized for binary
data.  That ensures maximum performance for I/O operation.

carray objects bring several advantages over NumPy::

  * Data is compressed: they take less memory space.

  * Efficent appends: you can append more data at the end of the
    objects very quickly.

  * Numexpr-powered: so that you can operate with compressed data in a
    fast and convenient way.


carray limitations
------------------

At this time, carray objects can only be uni-dimensional.  Newer
versions of the carray package might get rid of this limitation
though.


Installation
============

carray depends on NumPy (>= 1.4.1) and, optionally, Numexpr (>= 1.4).
Also, if you are going to install from sources, you will need Cython
(>= 0.13) and a C compiler (GCC or MSVC 2008 have been tested).

Installing from sources
-----------------------

Go to the carray main directory and do the typical distutils dance:

$ python setup.py install

Installing from Windows binaries
--------------------------------

Just download the binary installer and run it.


carray short tutorial
=====================

During all this tutorial, it is supposed that you are importing NumPy
and carray packages like this::

  >>> import numpy as np
  >>> import carray as ca

Creating carrays
----------------

A carray can be created from any NumPy ndarray by using its `carray`
constructor::

  >>> a = np.arange(10)
  >>> b = ca.carray(a)

Now, `b` is a carray object.  Just check this::

  >>> type(b)
  <type 'carray.carrayExtension.carray'>

You can have a peek at it by using its string form::

  >>> print b
  [0, 1, 2... 7, 8, 9]

And get more info about uncompressed size (nbytes), compressed
(cbytes) and the compression ratio (ratio = nbytes/cbytes), by using
its representation form::

  >>> b   # <==> print repr(b)
  carray((10,), int64)  nbytes: 80; cbytes: 4096; ratio: 0.02
  [0 1 2 3 4 5 6 7 8 9]

As you can see, the compressed size is much larger than the
uncompressed one.  How this can be?  Well, it turns out that carray
wears an I/O buffer for accelerating some internal operations.  So,
for small arrays (typically those taking less than 1 MB), there is
little point in using a carray.

However, when creating carrays larger than 1 MB (its natural
scenario), the size of the I/O buffer is generally negligible in
comparison::

  >>> a = np.arange(1e7)
  >>> b = ca.carray(np.arange(1e7))
  >>> b
carray((10000000,), float64)  nbytes: 76.29 MB; cbytes: 2.57 MB; ratio: 29.72
  cparms := cparms(clevel=5, shuffle=True)
[0.0, 1.0, 2.0, ..., 9999997.0, 9999998.0, 9999999.0]

You can always get a hint on how much space it takes your carray by
using `sys.getsizeof()`::

  >>> import sys
  >>> sys.getsizeof(b)
  2691754

Compression level and shuffle filter
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

carray uses Blosc as the internal compressor, and Blosc can be
directed to use different compression levels and to use (or not) its
internal shuffle filter.  The shuffle filter is a way to improve
compression when using items that have type sizes > 1 byte, although
it might be counter-productive (very rarely) in some data
distributions.

By default carrays are compressed using Blosc with compression level 5
with shuffle active.  But depending on you needs, you can use other
compression levels too::

  >>> ca.carray(a, ca.cparms(clevel=1))
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 10279111; ratio: 7.78
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

  >>> ca.carray(a, ca.cparms(clevel=9))
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 1088152; ratio: 73.52
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

Also, you can decide if you want to disable the shuffle filter that
comes with Blosc::

  >>> ca.carray(a, ca.cparms(shuffle=False))
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 38203113; ratio: 2.09
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

but, as you can see, the compression ratio is much worse in this case.
In general it is recommend to let shuffle active (unless you are
fine-tuning the performance for an specific carray).

See ``Optimization tips`` section for info on how you can change other
internal parameters like the size of the chunk.

Accessing carray data
---------------------

The way to access carray data is very similar to the NumPy indexing
scheme, that is, by specifying an index or slice::

  >>> a = np.arange(10)
  >>> b = ca.carray(a)
  >>> b[0]
  0
  >>> b[-1]
  9
  >>> b[2:4]
  array([2, 3])
  >>> b[4:]
  array([4, 5, 6, 7, 8, 9])
  >>> b[4:-2]
  array([4, 5, 6, 7])
  >>> b[:]
  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

As you see, NumPy objects are returned as the result of an indexing
operation.  You can even specify steps::

  >>> b[::2]
  array([0, 2, 4, 6, 8])
  >>> b[3:9:3]
  array([3, 6])

although negative steps are not supported::

  >>> b[3:9:-3]
  Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
      File "carrayExtension.pyx", line 395, in carray.carrayExtension.carray.__getitem__ (carray/carrayExtension.c:3327)
      NotImplementedError: step in slice can only be positive

carray metadata
---------------

carray implements a couple of attributes, `dtype` and `shape` that
makes it to 'quack' like a NumPy array::

  >>> a = np.arange(1e7)
  >>> b = ca.carray(a)
  >>> b.dtype
  dtype('float64')
  >>> b.shape
  (10000000,)

The fact that a carray 'quacks' like a NumPy array is important
because it can be used in some circumstances (but not always) where a
NumPy is expected (see ``Operating with carrays`` section below).

In addition, it implements the `cbytes` attribute that tells how many
bytes in memory uses the carray object::

  >>> b.cbytes
  2691722

This figure is approximate (the real one is a little larger) and it is
generally lower than the original (uncompressed) datasize can be
accessed by using `nbytes` attribute::

  >>> b.nbytes
  80000000

which is the same than the original NumPy array::

  >>> a.size*a.dtype.itemsize
  80000000

Finally, you can access the `chunklen` for this carray::

  >>> b.chunklen
  16384

Enlarging your carray
---------------------

One of the nicest features of carray objects is that they can be
enlarged very efficiently.  This can be done via the `carray.append()`
method.

For example, if `b` is a carray with 10 million elements::

  >>> b
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 2691722; ratio: 29.72
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

it can be enlarged by 10 elements with::

  >>> b.append(np.arange(10.))
  10   # the number of rows appended
  >>> b
  carray((10000010,), float64)  nbytes: 80000080; cbytes: 2691722; ratio: 29.72
  [0.0, 1.0, 2.0... 7.0, 8.0, 9.0]

You may want to compare the `carray.append()` method of carrays to the
`numpy.concatenate()` function in NumPy that serves similar purposes
with the 'bench/concat.py' script.  In general, you will see that the
`carray.append()` method is faster (specially with modern CPUs, that
can accelerate the compression/decompression process a lot).


Short Reference
===============

Utility functions
-----------------

* blosc_set_num_threads(nthreads)
    Set the number of threads that Blosc can use.

    Returns the previous setting for this number.

* blosc_version()
    Return the version of the Blosc library.

* detect_number_of_cores()
    Detect the number of cores on a system.


Optimization tips
=================

Changing explictly the length of chunks
---------------------------------------

You may want to use explictly the `chunklen` parameter to fine-tune
your compression levels::

  >>> a = np.arange(1e7)
  >>> ca.carray(a)
  carray((10000000,), float64)  nbytes: 76.29 MB; cbytes: 2.57 MB; ratio: 29.72
    cparms := cparms(clevel=5, shuffle=True)
  [0.0, 1.0, 2.0, ..., 9999997.0, 9999998.0, 9999999.0]
  >>> ca.carray(a).chunklen
  16384   # 128 KB = 16384 * 8 is the default chunk size for this carray
  >>> ca.carray(a, chunklen=512)
  carray((10000000,), float64)  nbytes: 76.29 MB; cbytes: 10.20 MB; ratio: 7.48
    cparms := cparms(clevel=5, shuffle=True)
  [0.0, 1.0, 2.0, ..., 9999997.0, 9999998.0, 9999999.0]
  >>> ca.carray(a, chunklen=8*1024)
  carray((10000000,), float64)  nbytes: 76.29 MB; cbytes: 1.50 MB; ratio: 50.88
    cparms := cparms(clevel=5, shuffle=True)
  [0.0, 1.0, 2.0, ..., 9999997.0, 9999998.0, 9999999.0]

As you can see, the length of the chunk affects very much compression
levels and the performance of I/O to carrays too.

In general, however, it is safer (and quicker!) to use the
`expectedlen` parameter (see next section).

Informing about the length of your carrays
------------------------------------------

If you are going to add a lot of rows to your carrays, be sure to use
the `expectedlen` parameter in creating time to inform the constructor
about the expected length of your final carray.  This allows carray to
fine-tune the length of its chunks more easily::

  >>> a = np.arange(1e7)
  >>> ca.carray(a, expectedlen=10).chunklen
  512
  >>> ca.carray(a, expectedlen=10*1000).chunklen
  4096
  >>> ca.carray(a, expectedlen=10*1000*1000).chunklen
  16384
  >>> ca.carray(a, expectedlen=10*1000*1000*1000).chunklen
  131072

carray iterators
----------------

It is good to know that carray implements very fast iterators::

  In [3]: a = np.arange(1e6)

  In [4]: time sum((v for v in a if v < 4))
  CPU times: user 0.75 s, sys: 0.00 s, total: 0.75 s
  Wall time: 0.77 s
  Out[5]: 6.0

  In [6]: b = ca.carray(a)

  In [7]: time sum((v for v in b if v < 4))
  CPU times: user 0.09 s, sys: 0.00 s, total: 0.09 s
  Wall time: 0.10 s   # 7.7x faster than NumPy iterator
  Out[8]: 6.0

You can also specify start, stop and step values, as well as adding
Python-space conditions::

  In [9]: time sum((v for v in a[2::3] if v < 10))
  CPU times: user 0.25 s, sys: 0.00 s, total: 0.25 s
  Wall time: 0.25 s
  Out[10]: 15.0

  In [11]: time sum((v for v in b.iter(start=2, step=3) if v < 10))
  CPU times: user 0.04 s, sys: 0.00 s, total: 0.04 s
  Wall time: 0.04 s   # 6.3x faster than NumPy
  Out[12]: 15.0

You may notice that this iterator is still faster than expressing
the condition in NumPy using fancy indexing::

  In [15]: time sum(a[a>10])
  CPU times: user 0.45 s, sys: 0.00 s, total: 0.45 s
  Wall time: 0.45 s
  Out[16]: 499999499945.0

  In [17]: time sum((v for v in b if v > 10))
  CPU times: user 0.13 s, sys: 0.00 s, total: 0.13 s
  Wall time: 0.13 s
  Out[18]: 499999499945.0

but far more slower than using a NumPy `sum` ufunc::

  In [20]: time a[a>10].sum()
  CPU times: user 0.01 s, sys: 0.00 s, total: 0.01 s
  Wall time: 0.01 s  # 13x faster than carray iterator
  Out[21]: 499999499945.0

But the advantage of the carray iterator is that you can use it in
generator contexts and hence, you don't need to waste memory for
creating temporaries, which can be important when dealing with large
arrays.



.. Local Variables:
.. mode: rst
.. coding: utf-8
.. fill-column: 70
.. End:
