=====================
carray short tutorial
=====================

During all this tutorial, it is supposed that you are importing NumPy
and carray packages like this::

  >>> import numpy as np
  >>> import carray as ca


Creating carrays
----------------

A carray can be created from any NumPy ndarray by using its `carray`
constructor::

  >>> a = np.arange(10)
  >>> b = ca.carray(a)

Now, `b` is a carray object.  Just check this::

  >>> type(b)
  <type 'carray.carrayExtension.carray'>

You can have a peek at it by using its string form::

  >>> print b
  [0, 1, 2... 7, 8, 9]


And get more info about uncompressed size (nbytes), compressed
(cbytes) and the compression ratio (ratio = nbytes/cbytes)::

  >>> b
  carray((10,), int64)  nbytes: 80; cbytes: 4096; ratio: 0.02
  [0 1 2 3 4 5 6 7 8 9]

As you can see, the compressed size is much larger than the
uncompressed one.  How this can be?  Well, it turns out that carray
wears an I/O buffer for accelerating some internal operations, and
this buffer is 4 KB for this case (but you can change its size by
using the `expectedrows` and `chunksize` parameters in the `carray`
constructor).  So, for small arrays (typically those taking less than
1 MB), there is little point in using a carray.

However, when creating carrays larger than 1 MB (its natural
scenario), the size of the I/O buffer is generally negligible in
comparison::

  >>> a = np.arange(1e7)
  >>> b = ca.carray(np.arange(1e7))
  >>> b
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 2691722; ratio: 29.72
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

You can always get a hint on how much space it takes your carray by
using `sys.getsizeof()`::

  >>> import sys
  >>> sys.getsizeof(b)
  2691754


Compression level and shuffle filter
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

carray uses Blosc as the internal compressor, and Blosc can be
directed to use different compression levels and to use (or not) its
internal shuffle filter.

By default carrays are compressed using Blosc with compression level 5
and shuffle is active.  But depending on you needs, you can use other
compression levels too::

  >>> ca.carray(a, ca.cparms(clevel=1))
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 10279111; ratio: 7.78
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

  >>> ca.carray(a, ca.cparms(clevel=9))
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 1088152; ratio: 73.52
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

Also, you can decide if you want to disable the shuffle filter that
comes with Blosc::

  >>> ca.carray(a, ca.cparms(shuffle=False))
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 38203113; ratio: 2.09
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

but, as you can see, the compression ratio is much worse in this case.
In general it is recommend to let shuffle active (unless you are
fine-tuning the performance for an specific carray).

You may want to use explictly the `chunksize` parameter to fine-tune
your compression levels::

  >>> ca.carray(a)
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 2691722; ratio: 29.72
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

  >>> ca.carray(a).chunksize
  131072   # 128 KB is the default chunksize for this carray

  >>> ca.carray(a, chunksize=4*1024)
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 10693440; ratio: 7.48
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

  >>> ca.carray(a, chunksize=32*1024)
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 2623992; ratio: 30.49
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

  >>> ca.carray(a, chunksize=64*1024)
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 1572375; ratio: 50.88
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

Finally, if you are going to add a lot of rows to your carrays, be
sure to use `expectedrows` parameter so that you inform the
constructor about the expected length of your final carray::

  >>> ca.carray(a, expectedrows=10).chunksize
  4096
  >>> ca.carray(a, expectedrows=10*1000).chunksize
  32768
  >>> ca.carray(a, expectedrows=10*1000*1000).chunksize
  131072
  >>> ca.carray(a, expectedrows=10*1000*1000*1000).chunksize
  1048576


Accessing carray data
---------------------

The way to access carray data is very similar to the NumPy indexing
scheme, that is, by specifying an index or slice::

  >>> a = np.arange(10)
  >>> b = ca.carray(a)
  >>> b[0]
  0
  >>> b[-1]
  9
  >>> b[2:4]
  array([2, 3])
  >>> b[4:]
  array([4, 5, 6, 7, 8, 9])
  >>> b[4:-2]
  array([4, 5, 6, 7])
  >>> b[:]
  array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

As you see, NumPy objects are returned as the result of an indexing
operation.  You can even specify steps::

  >>> b[::2]
  array([0, 2, 4, 6, 8])
  >>> b[3:9:3]
  array([3, 6])

although negative steps are not supported::

  >>> b[3:9:-3]
  Traceback (most recent call last):
    File "<stdin>", line 1, in <module>
      File "carrayExtension.pyx", line 395, in carray.carrayExtension.carray.__getitem__ (carray/carrayExtension.c:3327)
      NotImplementedError: step in slice can only be positive

Using iterators
~~~~~~~~~~~~~~~

carray also has support for efficient iterators::

  In [3]: a = np.arange(1e6)

  In [4]: time sum((v for v in a if v < 4))
  CPU times: user 6.51 s, sys: 0.00 s, total: 6.51 s
  Wall time: 6.52 s
  Out[5]: 6.0

  In [6]: b = ca.carray(a)

  In [7]: time sum((v for v in b if v < 4))
  CPU times: user 0.73 s, sys: 0.04 s, total: 0.78 s
  Wall time: 0.75 s  # 8.7x faster than ndarray
  Out[8]: 6.0

You can also specify start, stop and step values, as well as adding
Python-space conditions::

  In [9]: time sum((v for v in a[2::3] if v < 10))
  CPU times: user 2.18 s, sys: 0.00 s, total: 2.18 s
  Wall time: 2.19 s
  Out[10]: 15.0

  In [11]: time sum((v for v in b.iter(start=2, step=3) if v < 10))
  CPU times: user 0.26 s, sys: 0.03 s, total: 0.30 s
  Wall time: 0.30 s  # 7.3x faster than ndarray
  Out[12]: 15.0

You may notice that this iterator is still faster than expressing
the condition in NumPy using fancy indexing::

  In [3]: time sum(a[a>10])
  CPU times: user 1.78 s, sys: 0.02 s, total: 1.80 s
  Wall time: 1.80 s
  Out[4]: 499999499945.0

  In [5]: time sum((v for v in b if v > 10))
  CPU times: user 1.17 s, sys: 0.04 s, total: 1.22 s
  Wall time: 1.16 s   #  1.5x faster than fancy indexing
  Out[6]: 499999499945.0

but far more slower than using a NumPy `sum` ufunc::

  In [7]: time a[a>10].sum()
  CPU times: user 0.07 s, sys: 0.01 s, total: 0.08 s
  Wall time: 0.08 s   # 15x faster than carray
  Out[8]: 499999499945.0

However, the advantage of the carray iterator is that you can use it
in generator contexts and hence, you don't need to waste memory for
creating temporaries, which can be important when dealing with large
arrays.


carray metadata
---------------

carray implements a couple of attributes, `dtype` and `shape` that
makes it to 'quack' like a NumPy array::

  >>> a = np.arange(1e7)
  >>> b = ca.carray(a)
  >>> b.dtype
  dtype('float64')
  >>> b.shape
  (10000000,)

The fact that a carray 'quacks' like a NumPy array is important
because it can be used in some circumstances (but not always) where a
NumPy is expected (see ``Operating with carrays`` section below).

In addition, it implements the `cbytes` attribute that tells how many
bytes in memory uses the carray object::

  >>> b.cbytes
  2691722

This figure is approximate (the real one is a little larger) and it is
generally lower than the original (uncompressed) datasize can be
accessed by using `nbytes` attribute::

  >>> b.nbytes
  80000000

which is the same than the original NumPy array::

  >>> a.size*a.dtype.itemsize
  80000000

Finally, you can access the `chunksize` for this carray::

  >>> b.chunksize
  131072


Enlarging your carray
---------------------

One of the nicest features of carray objects is that they can be
enlarged very efficiently.  This can be done via the `carray.append()`
method.

For example, if `b` is a carray with 10 million elements::

  >>> b
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 2691722; ratio: 29.72
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

it can be enlarged by 10 elements with::

  >>> b.append(np.arange(10.))
  10   # the number of rows appended
  >>> b
  carray((10000010,), float64)  nbytes: 80000080; cbytes: 2691722; ratio: 29.72
  [0.0, 1.0, 2.0... 7.0, 8.0, 9.0]

You may want to compare the `carray.append()` method of carrays to the
`numpy.concatenate()` function in NumPy that serves similar purposes
with the 'bench/concat.py' script.  In general, you will see that the
`carray.append()` method is faster (specially with modern CPUs, that
can accelerate the compression/decompression process a lot).


Operating with carrays
----------------------

A carray is not only useful as a high-performance in-memory data
container, but you can also use it to perform operations with them by
using the `tables.Expr` class that comes with PyTables (>= 2.2).

`tables.Expr` uses the Numexpr package under the covers, so operations
are both memory-efficient and multi-threaded (i.e. they can use
multiple cores, if available).  If we add the high-performance Blosc
compressor (that is used by carray internally) into this equation,
carray allows for an extremely efficient way to perform computations
that saves both memory and time, specially on very large arrays.

For example, let's create a carray::

  >>> a = np.arange(1e7)
  >>> b = ca.carray(a)
  >>> b
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 3590458; ratio: 22.28
  [0.0, 1.0, 2.0... 9999997.0, 9999998.0, 9999999.0]

Now, let's add 1 to it::

  >>> import tables as tb
  >>> expr = tb.Expr("b+1")
  >>> cout = ca.carray(np.empty((0,), dtype='f8'))
  >>> expr.setOutput(cout, append_mode=True)
  >>> expr.eval()
  carray((10000010,), float64)  nbytes: 80000080; cbytes: 3599988; ratio: 22.22
  [1.0, 2.0, 3.0... 8.0, 9.0, 10.0]   # returns cout, as can be seen in:
  >>> cout
  carray((10000010,), float64)  nbytes: 80000080; cbytes: 3599988; ratio: 22.22
  [1.0, 2.0, 3.0... 8.0, 9.0, 10.0]  # same object than above

where `cout` holds the results of the operation.  Initially we have
created it as an empty carray, and we instructed the `expr` instance
to add results to it (by using its `append()` mode under the hood).

Of course, you can evaluate more complex expressions and mix carrays
and NumPy arrays (as well as other PyTables objects) freely::

  >>> expr = tb.Expr("a**2+b**3+sin(a*b)-cout")
  >>> cout2 = ca.carray(np.empty((0,), dtype='f8'))
  >>> expr.setOutput(cout2, append_mode=True)
  >>> expr.eval()
  carray((10000000,), float64)  nbytes: 80000000; cbytes: 40654959; ratio: 1.97
  [-1.0, 0.84147098480789673, 8.2431975046920716... 9.9999920000020002e+20, 9.9999950000007007e+20, 9.9999980000000016e+20]

In the expression above we have included a couple of carray instances
(`b` and `cout`) with a pure NumPy array (`a`), and the result is a
new carray (`cout2`).


Utility functions
-----------------

* blosc_set_num_threads(nthreads)
    Set the number of threads that Blosc can use.

    Returns the previous setting for this number.

* blosc_version()
    Return the version of the Blosc library.

* detect_number_of_cores()
    Detect the number of cores on a system.


carray limitations
------------------

At this time, carray objects can only be uni-dimensional and you cannot
modify its contents directly.  Newer versions of the carray package
might get rid of these limitations though.


.. Local Variables:
.. mode: rst
.. coding: utf-8
.. fill-column: 70
.. End:
